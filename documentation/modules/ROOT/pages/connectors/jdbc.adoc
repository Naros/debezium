[id="debezium-connector-for-jdbc"]
= {prodname} connector for JDBC
:context: JDBC
:mbean-name: {context}
:toc:
:toc-placement: macro
:linkattrs:
:icons: font
:source-highlighter: highlight.js

toc::[]

[NOTE]
====
This connector is currently in incubating state, i.e. exact semantics, configuration options etc. may change in future revisions, based on the feedback we receive.
Please let us know if you encounter any problems.
====

[[jdbc-overview]]
== Overview

{prodname}'s JDBC connector is a Kafka Connect sink connector implementation that allows you to consume from any number of source topics and write the events from those topics to any relational database using a JDBC driver.
This connector supports a wide variety of database dialects, including Db2, MySQL, PostgreSQL, Oracle, and SQL Server.

[[how-the-jdbc-connector-works]]
== How the JDBC connector works

The {prodname} JDBC connector is a Kafka Connect sink connector, and therefore requires the Kafka Connect runtime.
The JDBC connector periodically polls the Kafka topics that it is subscribed, and consumes events from those topics and writes them to the configured relational database.
The connector supports idempotent write operations by using upsert semantics and basic schema evolution.

The following features are supported:

  * xref:jdbc-consume-complex-debezium-events[]
  * xref:jdbc-at-least-once-delivery[]
  * xref:jdbc-multiple-tasks[]
  * xref:jdbc-data-and-type-mappings[]
  * xref:jdbc-primary-key-handling[]
  * xref:jdbc-delete-mode[]
  * xref:jdbc-idempotent-writes[]
  * xref:jdbc-schema-evolution[]
  * xref:jdbc-quoting-case-sensitivity[]

[[jdbc-consume-complex-debezium-events]]
=== Consuming complex {prodname} change events

The {prodname} source connectors produce complex, hierarchical change events by default.
When working with other JDBC sink connector implementations, it is often necessary to use the `ExtractNewRecordState` single message transformation in order to flatten the payload of the change event so that it can be consumed by the sink implementation.
The {prodname} JDBC sink connector does not have this requirement, so you can safely consume native {prodname} change events directly without a transformation.

When the JDBC sink connector consumes {prodname}'s complex change events, the `after` section for insert or update events will be used as the basis for the event's values.
When a delete event is consumed by the sink connector, no part of the event's payload is consulted.

[[jdbc-at-least-once-delivery]]
=== At least once delivery

The {prodname} JDBC sink connector guarantees that events that are consumed from Kafka topics are processed at least once.

[[jdbc-multiple-tasks]]
=== Multiple tasks

The {prodname} JDBC sink connector supports execution across multiple Kafka Connect tasks.
In order to execute the connector across multiple tasks, you can specify the `tasks.max` configuration property and multiple tasks will be started by the Kafka Connect runtime.
Multiple tasks can improve performance by reading and processing changes from multiple source topics in parallel.

[[jdbc-data-and-type-mappings]]
=== Data and column type mappings

The {prodname} JDBC sink connector requires an understanding of the data types associated with a given field/column in the source event.
Each database dialect supported by this connector has a wide range of column type mappings that it leverages to resolve the destination column type from the event field's type metadata.
Resolution of a column's type can be enhanced by enabling the `column.propagate.source.type` or the `datatype.propagate.source.type` source connector configuration options.
The extra parameter metadata that {prodname} includes when enabling these features can allow the JDBC sink connector to resolve the destination column's type more accurately.

The {prodname} JDBC sink connector expects that the Kafka topic message key, when present, either be a primitive data type or a `Struct`.
The message payload must be a `Struct` that either adheres to a flattened structure where there are no nested `struct` types or that the nested `struct` layout conform to {prodname}'s complex, hierarchical structure.

If the structure of the events in the Kafka topic do not adhere to these rules, a custom `Transformation` will be required to adapt the source event structure accordingly.

[[jdbc-primary-key-handling]]
=== Primary key handling

By default, the {prodname} JDBC sink connector does not associate any fields from the source event to a primary key.
Unfortunately, this is not always a good choice depending on the business requirements and configuration of the sink connector, such as using upsert semantics.
The connector can be configured with several different primary key modes:

|===
|Mode|Description

|`none`
|No primary key fields are specified when creating the table.

|`kafka`
|The primary key will consist of three columns `\__connect_topic`, `__connect_partition`, and `__connect_offset`.
The values for these columns will be sourced from the Kafka event's coordinates.

|`record_key`
|The primary key will be composed of the Kafka event's key. +
 +
If the primary key is a primitive type, the `primary.key.fields` property must be specified, which specifies the name of the column to be used.
If the primary key is a struct type, the fields in the struct will be mapped as columns of the primary key.  The `primary.key.fields` property can be used to restrict the primary key to a subset of columns.

|`record_value`
|The primary key will be composed of the Kafka event's value. +
 +
Since the Kafka event's value must always be a `Struct`, all fields within the value will be used as primary key columns by default.
In order to use a subset of fields, specify the `primary.key.fields` property with a comma-separated list of fields from the value to derive the primary key columns of.

|===

[IMPORTANT]
====
Some dialects may throw an exception if you set the `primary.key.mode` to `kafka` and set `schema.evolution` to `basic`.
This exception occurs because some dialects map a `STRING` data type mapping to a variable length string data type such as `TEXT` or `CLOB`, and these dialects do not allow unbounded length primary key columns.
To avoid this problem, the following must be set:

* Do not set `schema.evolution` to `basic`.
* Create the database table and primary key mappings in advance.
====

[[jdbc-delete-mode]]
=== Delete mode

The {prodname} JDBC sink connector can delete rows in the destination database when a `DELETE` or _tombstone_ event is consumed.
By default, the JDBC sink connector does not enable delete mode, therefore, it must be enabled explicitly in the connector configuration if you wish to support removing rows.

Deletes can be enabled by setting `delete.enabled=true` in the connector configuration, but this mode requires that the `primary.key.mode` be set to a value that is not `none`.
This is because deletes are executed based on the primary key mapping, and therefore is a destination table has no primary key mapping, rows cannot be deleted by the connector.

[[jdbc-idempotent-writes]]
=== Idempotent writes

The {prodname} JDBC sink connector supports idempotent writes, allowing the same records to be replayed repeatedly and the final database state remains consistent.
In order to support idempotent writes, the JDBC sink connector must be explicitly configured with the `insert.mode` set to `upsert`.
An _upsert_ operation is one where the operation performed is effectively an insert if the row does not already exist, otherwise an update of the row will be applied if the row does exist.

Each database dialect handles idempotent writes differently because there is no SQL standard for _upsert_ operations.
The following illustrates the upsert database-specific DML syntax that is used by the support dialects:

|===
|Dialect |Upsert Syntax

|Db2
|`MERGE ...`

|MySQL
|`INSERT ... ON DUPLICATE KEY UPDATE ...`

|Oracle
|`MERGE ...`

|PostgreSQL
|`INSERT ... ON CONFLICT ... DO UPDATE SET ...`

|SQL Server
|`MERGE ...`

|===

[[jdbc-schema-evolution]]
=== Schema evolution

The {prodname} JDBC sink connector supports the following schema evolution modes:

|===
|Mode |Description

|`none`
|The connector does not perform any DDL schema evolution.

|`basic`
|The connector automatically detects fields that are in the event payload but do exist in the destination table and alters the table, adding the new fields.

|===

When `schema.evolution` is set to `basic`, the connector will automatically create or alter the destination database table according to the structure of the incoming event.

When an event is received from a topic for the first time and the destination table does not yet exist, the {prodname} JDBC sink connector will use the event's key and/or record schema structure to resolve the column structure of the table.
When schema evolution is enabled, a `CREATE TABLE` SQL statement will be prepared and executed by the connector before applying the DML event to the destination table.

When an event is received from a topic and the record schema structure differs from the destination table , the {prodname} JDBC sink connector will use the event's key and/or record schema structure to resolve which columns are new that need to be added to the database table.
When schema evolution is enabled, an `ALTER TABLE` SQL statement will be prepared and executed by the connector before applying the DML event to the destination table.
The changing of column data types, dropping of columns, and adjustments to primary keys can be considered dangerous, and therefore none of these operations are performed by the connector.

Whether a column is `NULL` or `NOT NULL` is derived directly from the optionality defined in the field's schema, and default values are also resolved from the field schema as well.
If the connector attempts to create a table with an undesired nullability setting or default value, the table will either need to be created manually ahead of time or the schema of the associated field adjusted prior to be processing by the sink connector.
This can be accomplished by introducing a custom single message transformation that adjusts these in the pipeline or modifying the column state in the source database.

A field's data type is resolved based on  a predefined set of mappings, which can be found in the xref:jdbc-field-types[] section.

[IMPORTANT]
====
When introducing new fields to the event structure for tables that already exist in the destination database, the new fields must be defined as optional or have a default value specified in its schema.
If a field needs to be removed from the destination table, it should be removed manually to either drop the column, assign it a default value, or make it nullable.
====

[[jdbc-quoting-case-sensitivity]]
=== Quoting and case sensitivity

The {prodname} JDBC sink connector consumes Kafka messages by constructing either DDL (schema changes) or DML (data changes) SQL statements that are executed on the destination database.
The default behavior is to use the topic and field names as the basis for the table and column names of the relational table.
The constructed SQL does not automatically quote identifiers, and so by default the case of the table or column names is fully dependent on the destination database's behavior.

For example, if the destination database dialect is Oracle and the event's topic is `orders`, the destination table will be created as `ORDERS` because Oracle defaults to upper-case names when the name is not quoted.
Similarly, if the destination database dialect is PostgreSQL and the event's topic is `ORDERS`, the destination table will be created as `orders` because PostgreSQL defaults to lower-case names when the name is not quoted.

By setting `quote.identifiers` to `true` in the connector configuration, the case of the table and field names can be kept explicitly set to match the case present in the Kafka event.
So if the incoming event is for a topic called `orders` and the destination database dialect is Oracle, when quoting is enabled the table will be created as `orders` since the constructed SQL will use `"orders"` as the name of the table.
When quoting is enabled, the behavior for column names works identical.

[[jdbc-field-types]]
== Data type mappings

The {prodname} JDBC sink connector resolves a column's data type by using a logical or primitive type mapping system.
Primitive types include values like integers, floating points, booleans, strings, and bytes.
These are typically represented  with a specific Kafka Connect `Schema` type code only.
Logical types are more often complex types, including values such as `Struct`-based types that have a fixed set of field names and schema or values that are represented with a specific encoding such as number of days since epoch.

The following are examples of primitive and logical types:

.Primitive field schema
[source.json]
----
{
  "schema": {
    "type": "INT64"
  }
}
----

.Logical field schema
[source,json]
----
[
  "schema": {
    "type": "INT64",
    "name": "org.apache.kafka.connect.data.Date"
  }
]
----

Kafka Connect is not the only source for these complex, logical types.
In fact, {prodname} source connectors also generate fields in change events that  have similar logical types to represent a variety of different data types, including but not limited to, timestamps, dates, and even JSON data.

The {prodname} JDBC sink connector uses these primitive and logical types to resolve a column's type to a JDBC SQL code, which represents a column's type.
These JDBC SQL codes are then used by the underlying Hibernate persistence framework to resolve the column's type to a logical data type for the dialect in use.
The following tables illustrate the these primitive and logical mappings for both Kafka Connect and Debezium to the JDBC SQL types.
The actual final column type then varies based on the database.

. xref:#jdbc-kafka-connect-primitive-mappings[]
. xref:#jdbc-kafka-connect-logical-mappings[]
. xref:#jdbc-debezium-logical-mappings[]
. xref:#jdbc-debezium-logical-mappings-dialect-specific[]

[[jdbc-kafka-connect-primitive-mappings]]
.Kafka Connect Primitives to Column Data Type Mappings
|===
|Primitive Type |JDBC SQL Type

|INT8
|Types.TINYINT

|INT16
|Types.SMALLINT

|INT32
|Types.INTEGER

|INT64
|Types.BIGINT

|FLOAT32
|Types.FLOAT

|FLOAT64
|Types.DOUBLE

|BOOLEAN
|Types.BOOLEAN

|STRING
|Types.CHAR, Types.NCHAR, Types.VARCHAR, Types.NVARCHAR

|BYTES
|Types.VARBINARY

|===

[[jdbc-kafka-connect-logical-mappings]]
.Kafka Connect Logical Type to Column Data Type Mappings
|===
|Logical Type |JDBC SQL Type

|org.apache.kafka.connect.data.Decimal
|Types.DECIMAL

|org.apache.kafka.connect.data.Date
|Types.DATE

|org.apache.kafka.connect.data.Time
|Types.TIMESTAMP

|org.apache.kafka.connect.data.Timestamp
|Types.TIMESTAMP

|===

[[jdbc-debezium-logical-mappings]]
.{prodname} Logical Type to Column Data Type Mappings
|===
|Logical Type |JDBC SQL Type

|io.debezium.time.Date
|Types.DATE

|io.debezium.time.Time
|Types.TIMESTAMP

|io.debezium.time.MicroTime
|Types.TIMESTAMP

|io.debezium.time.NanoTime
|Types.TIMESTAMP

|io.debezium.time.ZonedTime
|Types.TIME_WITH_TIMEZONE

|io.debezium.time.Timestamp
|Types.TIMESTAMP

|io.debezium.time.MicroTimestamp
|Types.TIMESTAMP

|io.debezium.time.NanoTimestamp
|Types.TIMESTAMP

|io.debezium.time.ZonedTimestamp
|Types.TIMESTAMP_WITH_TIMEZONE

|io.debezium.data.VariableScaleDecimal
|Types.DOUBLE

|===

[IMPORTANT]
====
If the database does not support time or timestamps with time zones, the mapping will resolve to its equivalent without timezones.
====

[[jdbc-debezium-logical-mappings-dialect-specific]]
.{prodname} dialect-specific Logical Type to Column Data Type Mappings
|===
|Logical Type |MySQL SQL Type |PostgreSQL SQL Type |SQL Server SQL Type

|io.debezium.data.Bits
|`bit(n)`
|`bit(n)` or `bit varying`
|`varbinary(n)`

|io.debezium.data.Enum
|`enum`
|Types.VARCHAR
|n/a

|io.debezium.data.Json
|`json`
|`json`
|n/a

|io.debezium.data.EnumSet
|`set`
|n/a
|n/a

|io.debezium.time.Year
|`year(n)`
|n/a
|n/a

|io.debezium.time.MicroDuration
|n/a
|`interval`
|n/a

|io.debezium.data.Ltree
|n/a
|`ltree`
|n/a

|io.debezium.data.Uuid
|n/a
|`uuid`
|n/a

|io.debezium.data.Xml
|n/a
|`xml`
|`xml`

|===

In addition to the primitive and logical mappings above, if the source of the change events is a {prodname} source connector, the resolution of the column type, its length, precision, and scale can be further influenced by enabling column or data type propagation.
This propagation requires that the source connector be configured with `column.propagate.source.type` or `datatype.propagate.source.type` and the {prodname} JDBC sink connector will use these values with higher precedence.

For example, let's say the following field schema is included in a change event:

.{prodname} change event field schema with column/data type propagation enabled
[source,json]
----
{
  "schema": {
    "type": "INT8",
    "parameters": {
      "__debezium.source.column.type": "TINYINT",
      "__debezium.source.column.length": "1"
    }
  }
}
----

In the above example, without the schema parameters, the {prodname} JDBC sink connector would map this field to a column type of `Types.SMALLINT`, which will have varying logical database types depending on the dialect but for MySQL this would be a `TINYINT` column type without any length.
By enabling the column or data type propagation in the source connector, the {prodname} JDBC sink connector can use this information to refine the data type mapping process and will create the column as `TINYINT(1)`.

[NOTE]
====
Typically, column / data type propagation will have a much larger influence when both the source and sink databases are the same.
We are continually looking at ways to improve this mapping across heterogeneous databases and the current type system allows us to continue to refine these mappings based on feedback.
If you find a mapping could be improved, please let us know.
====

[[jdbc-deployment]]
== Deployment

To deploy a {prodname} JDBC connector, you install the {prodname} JDBC connector archive, configure the connector, and start the connector by adding its configuration to Kafka Connect.

.Prerequisites
* link:https://zookeeper.apache.org/[Apache ZooKeeper], link:http://kafka.apache.org/[Apache Kafka], and link:{link-kafka-docs}.html#connect[Kafka Connect] are installed.
* Destiniation database is installed and configured to accept JDBC connections.

.Procedure

. Download the {prodname} https://repo1.maven.org/maven2/io/debezium/debezium-connector-jdbc/{debezium-version}/debezium-connector-oracle-{debezium-version}-plugin.tar.gz[JDBC connector plug-in archive].
. Extract the files into your Kafka Connect environment.
. Optionally download the JDBC driver from Maven Central and extract the downloaded driver file to the directory that contains the JDBC sink connector JAR file.
+
NOTE: Drivers for Oracle and Db2 are not automatically shipped with the JDBC sink connector and must be manually installed.
. Add the directory with the JAR files to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`].
. Restart your Kafka Connect process to pick up the new JAR files.

[[jdbc-connector-configuration]]
=== {prodname} JDBC connector configuration

Typically, you register a {prodname} JDBC connector by submitting a JSON request that specifies the configuration properties for the connector.
The following example shows a JSON request for registering an instance the {prodname} JDBC sink connector that consumes events from a topic called `orders` with the most common configuration settings:

.Example: {prodname} JDBC connector configuration
[source,json,indent=0,subs="+quotes"]
----
{
    "name": "jdbc-connector",  // <1>
    "config": {
        "connector.class": "io.debezium.connector.jdbc.JdbcSinkConnector",  // <2>
        "tasks.max": "1",  // <3>
        "connection.url": "jdbc:postgresql://localhost/db",  // <4>
        "connection.username": "pguser",  // <5>
        "connection.password": "pgpassword",  // <6>
        "insert.mode": "upsert",  // <7>
        "delete.enabled": "true",  // <8>
        "primary.key.mode": "record_key",  // <9>
        "schema.evolution": "basic",  // <10>
        "database.time_zone": "UTC"  // <11>
    }
}
----
<1> The name that is assigned to the connector when you register it with Kafka Connect service.
<2> The name of the JDBC sink connector class.
<3> The maximum number of tasks to create for this connector.
<4> The JDBC URL used to connect to the database where events will be written by this connector.
<5> The name of the database user used for authentication.
<6> The password of the database user used for authentication.
<7> The insert mode used by the connector, see the xref:#jdbc-property-insert-mode[insert.mode] configuration property.
<8> Enables the ability to delete records in the database, see the xref:#jdbc-property-delete-enabled[delete.enabled] configuration property.
<9> Specifies the method used to resolve primary key columns, see the xref:#jdbc-property-primary-key-mode[primary.key.mode] configuration property.
<10> Enables the connector to evolve the destination database's schema, see the xref:#jdbc-property-schema-evolution[schema.evolution] configuration property.
<11> Specifies the timezone used when writing temporal field types.

For a complete list of configuration properties that you can set for the {prodname} JDBC connector, see xref:#jdbc-connector-properties[JDBC connector properties].

You can send this configuration with a `POST` command to a running Kafka Connect service.
The service records the configuration and starts a sink connector task(s) that performs the following operations:

* Connects to the database
* Consumes events from subscribed Kafka topics
* Writes the events to the configured database.

[[jdbc-connector-properties]]
== Connector properties

The {prodname} JDBC sink connector has several configuration properties that you can use to achieve the right connector behavior for your needs.
Many properties have default values.
Information about the properties is organized as follows:

* xref:jdbc-connector-properties-connection[]
* xref:jdbc-connector-properties-runtime[]
* xref:jdbc-connector-properties-extendable[]

[[jdbc-connector-properties-connection]]
.Connection properties
[cols="30%a,25%a,45%a"]
|===
|Property |Default |Description

|[[jdbc-property-connection-url]]<<jdbc-property-connection-url, `+connection.url+`>>
|No default
|The JDBC connection URL used to connect to the database.

|[[jdbc-property-connection-username]]<<jdbc-property-connection-username, `+connection.username+`>>
|No default
|The name of the database user account that the connector uses to connect to the database.

|[[jdbc-property-connection-password]]<<jdbc-property-connection-password, `+connection.password+`>>
|No default
|Password to use when connecting to the database.

|[[jdbc-property-connection-pool-min-size]]<<jdbc-property-connection-pool-min-size, `+connection.pool.min_size+`>>
|`5`
|Specifies the minimum number of connections the pool will maintain.

|[[jdbc-property-connection-pool-max-size]]<<jdbc-property-connection-pool-max-size, `+connection.pool.min_size+`>>
|`32`
|Specifies the maximum number of connections the pool will maintain at any given time.

|[[jdbc-property-connection-pool-acquire-increment]]<<jdbc-property-connection-pool-acquire-increment, `+connection.pool.acquire_increment+`>>
|`32`
|Specifies the number of connections to try and acquire when out of available connections.

|[[jdbc-property-connection-pool-timeout]]<<jdbc-property-connection-pool-timeout, `+connection.pool.timeout+`>>
|`1800`
|Specifies the number of seconds that an unused connection is kept before discarded.

|===

[[jdbc-connector-properties-runtime]]
.Runtime properties
[cols="30%a,25%a,45%a"]
|===
|Property |Default |Description

|[[jdbc-property-database-time-zone]]<<jdbc-property-database-time-zone, `+database.time_zone+`>>
|`UTC`
|Specifies the timezone used when inserting JDBC temporal values.

|[[jdbc-property-delete-enabled]]<<jdbc-property-delete-enabled, `+delete.enabled+`>>
|`false`
|Specifies whether the connector will process `DELETE` or _tombstone_ events and remove the corresponding row from the database.  This requires that the `primary.key.mode` option be set to `record.key`.

|[[jdbc-property-insert-mode]]<<jdbc-property-insert-mode, `+insert.mode+`>>
|`insert`
|Specifies the strategy used to insert events into the database.

`insert`:: Specifies that all events should construct `INSERT`-based SQL statements.
This should only be used when its guaranteed there will never be updates to previously inserted rows with the same primary key or when no primary key is used.
`update`:: Specifies that all events should construct `UPDATE`-based SQL statements.
This should only be used when its guaranteed there will only be events for already existing rows.
`upsert`:: Specifies that events will be added to the table using upsert semantics, meaning that if the primary key does not exist, the connector will use an `INSERT` and if the key does exist, the connector will use an `UPDATE`.
When idempotent writes are required, the connector should be configured with this insert mode.

|[[jdbc-property-primary-key-mode]]<<jdbc-property-primary-key-mode, `+primary.key.mode+`>>
|`none`
|Specifies how the primary key columns will be resolved from the event.

`none`:: Specifies that no primary key columns will be created.
`kafka`:: Specifies that the Kafka coordinates should be used as the primary key columns.
The key coordinates are defined from the topic name, partition, and offset of the event and mapped to columns named `\__connect_topic`, `__connect_partition`, and `__connect_offset`.
`record_key`:: Specifies that the primary key columns will be sourced from the event's record key.
If the record key is a primitive type, the `primary.key.fields` property is required to specify the name of the primary key column.
If the record key is a struct type, the `primary.key.fields` property is optional and can be used to specify a subset of columns from the event's key as the table's primary key.
`record_value`:: Specifies that the primary key columns will be sourced from the event's value.
The `primary.key.fields` property can be specified to set the primary key as a subset of fields from the event's value; otherwise all fields are used by default.

|[[jdbc-property-primary-key-fields]]<<jdbc-property-primary-key-fields, `+primary.key.fields+`>>
|No default
|Either the name of the primary key column or a comma-separated list of fields to derive the primary key from. +
 +
When `primary.key.mode` is set to `record_key` and the event's key is a primitive type, it is expected that this property specifies the column name to be used for the key. +
 +
When the `primary.key.mode` is either `record_key` with a non-primitive key or `record_value`, it is expected that this property specifies a comma-separated list of field names from either the key or value.
If the `primary.key.mode` is either `record_key` with a non-primitive key or `record_value` and this property is not specifies, the connector will use all fields as the primary key either from the key or value depending on the mode.

|[[jdbc-property-quote-identifiers]]<<jdbc-property-quote-identifiers, `+quote.identifiers+`>>
|`false`
|Specifies whether table and column names will be quoted in generated SQL statements.
See the xref:jdbc-quoting-case-sensitivity[] section for more details.

|[[jdbc-property-schema-evolution]]<<jdbc-property-schema-evolution, `+schema.evolution+`>>
|`none`
|Specifies how the connector will evolve the destination table schemas.
See the xref:jdbc-schema-evolution[] section for more details.

`none`:: Specifies that the connector will not evolve the destination schema.
`basic`:: Specifies that basic evolution will occur by adding any missing columns to the table by comparing the incoming event's record schema to the database table structure.

|[[jdbc-property-table-name-format]]<<jdbc-property-table-name-format, `+table.name.format+`>>
|`${topic}`
|Specifies a string that controls how the destination table name will be formatted based on the event's topic name.
The placeholder, `${topic}`, will be replaced by the topic name.

|===

[[jdbc-connector-properties-extendable]]
.Extendable properties
[cols="30%a,25%a,45%a"]
|===
|Property |Default |Description

|[[jdbc-property-column-naming-strategy]]<<jdbc-property-column-naming-strategy, `+column.naming.strategy+`>>
|`i.d.c.j.n.DefaultColumnNamingStrategy`
|Specifies the fully-qualified class name of a `ColumnNamingStrategy` implementation that the connector will use to resolve column names from event field names. +
 +
The default behavior is to simply use the field name as the column name.
|[[jdbc-property-table-naming-strategy]]<<jdbc-property-table-naming-strategy, `+table.naming.strategy+`>>
|`i.d.c.j.n.DefaultTableNamingStrategy`
|Specifies the fully-qualified class name of a `TableNamingStraetgy` implementation that the connector users to resolve table names from incoming event topic names. +
 +
The default behavior is to: +

* Replace the `${topic}` placeholder in the `table.name.format` configuration property with the event's topic.
* Sanitize the table name by replacing dots (`.`) with underscores (`_`).

|===

[[jdbc-faq]]
== Frequently asked questions

* *Is the* `ExtractNewRecordState` *single message transformation required?* +
No, that is actual one of the differentiating factors of the {prodname} JDBC connector from its competitors.
While the connector is capable of ingesting flattened events like its competitors, it can also ingest Debezium's complex change event structure natively, without requiring any specific type of transformation.

* *If a column's type is changed or a column is renamed or dropped, is this handled by schema evolution?* +
No, the {prodname} JDBC connector does not make any changes to existing columns.
The schema evolution supported by the connector is quite basic, it simply compares the fields in the event structure to the table's column list and simply adds any fields that are not yet defined as columns in the table.
If a column's type or default value change, this will not be adjusted by the connector in the destination database.
If a column is renamed, the old column will be left as is and a new column will be appended to the table with the new name; however existing rows with data in the old column will remain unchanged.
These types of schema changes should be handled manually.

* *A column's type is resolved to a type that is not desired, how can the type be resolved to a different data type?* +
The {prodname} JDBC connector uses a sophisticated type system to resolve a column's data type.
You can refer to the xref:#jdbc-data-and-type-mappings[] section for details on how this type system resolves a specific field's schema definition to a JDBC type.
If you desire a different data type mapping, you should define the table manually so you explicitly control the desired column type you need.

* *How do you specify a prefix or a suffix to the table name without changing the Kafka topic name?* +
In order to add a prefix or a suffix to the destination table  name, simply adjust the xref:#jdbc-property-table-name-format[table.name.format] connector configuration property and prepend or append the desired prefixed or suffix.
For example, to prefix all table names with `jdbc_`, specify the `table.name.format` configuration property with a value of `jdbc_${topic}`.
If such a connector is subscribed to a topic called `orders`, the resulting table will be created as `jdbc_orders`.

* *Even when identifier quoting is not enabled, some columns are automatically quoted, why?* +
There are situations where specific column or table names may still be explicitly quoted even when `quote.identifiers` is not enabled.
This is often necessary when the column or table name starts with or uses a specific convention that would otherwise be considered illegal syntax.
For example, when setting the xref:#jdbc-property-primary-key-mode[primary.key.mode] to `kafka`, certain databases will not allow a column's name to begin with an underscore unless the column's name is quoted.
This behavior is dialect specific and varies depending on the database used.

